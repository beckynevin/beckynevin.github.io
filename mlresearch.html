
<html>
  <head>
    <meta charset="UTF-8">
    <title>Becky Nevin</title>
    <link rel="stylesheet" href="css/screen.css" type="text/css" media="screen, projection">
    <link rel="stylesheet" href="css/print.css" type="text/css" media="print">
    <link rel="stylesheet" type="text/css" href="css/becky.css">
    <!--[if IE]><link rel="stylesheet" href="css/blueprint/ie.css" type="text/css" media="screen, projection"><![endif]-->
<script type="text/javascript" language="JavaScript">

</script>


</head>

  <body>
    <div class="container">
      <div class="column span-24 first last">
	<h1>Becky Nevin</h1>
      </div>
      <hr>
    </div>
    <div class="container">

      <div class="span-4 first colborder">
        <a href="index.html">home</a><br>
        <a href="mlresearch.html">machine learning research</a><br>
        <a href="galresearch.html">galaxy research</a><br>
        <a href="cv.html">resume and cv</a><br>
  	    <a href="outreach.html">science communication</a><br>
        <a href="talks.html">talks</a><br>
        <a href="stickers.html">stickers</a><br>
        <a href="misc.html">about me</a><br>
      </div>
      <div class="span-13">

<p><b>DeepUQ: Testing the fidelity of uncertainty predictions from deep learning methods</b>
  <p>One of my roles in the Deepskies group is to develop and test tools for benchmarking and assessing uncertainty predictions from 
  machine learning methods. I'm currently testing Deep Ensembles and Deep Evidential Regression, Deep Ensembles shown below:</p>
  <img src="files/website_MVEs.png" width=500 class="center">
  <p>My current focus is the aleatoric uncertainty shown above.</p>

<img src="files/design_high.png" width="400" class="center">

  <img src="files/sigma_in_sigma_out_ridgeplot_ensemble_two_panel_noise_color_15_uniform.png" width=400 class="center">
  <p><a href="https://github.com/deepskies/DeepUQ/tree/main">DeepUQ Github</a> 
  
  <hr>


<p><b>DeepDiagnostics: Assessing the quality of inference</b>
  <p>I created DeepDiagnostics, which is a software package for diagnosing the quality of inference for different inference techniques, ranging from likelihood-based techniques such as MCMC to simulation-based inference. Simulation-based inference is an ML-enabled technique to accelerate traditionally computationally expensive inference approaches. It achieves this speed up by approximating the likelihood using a normalizing flow or similar generative technique. </p>
  <p>Since the Deepskies lab has experience in SBI, I designed DeepDiagnostics to work in concert with mackelab's sbi software.</p>
  <p>It includes utilities that handle data generation, saving, and lookup, as well as evaluation and plotting utilities for flexible corner plots (below) and evaluation techniques such as coverage fraction (below):</p>
  <div style="display: inline-block;">
    <img src="files/website_corner.png" width="400" class="center">
  </div>
  <div style="display: inline-block;">
    <img src="files/website_coverage.png" width="400" class="center">
  </div> 
  <p><a href="https://github.com/deepskies/DeepDiagnostics">DeepDiagnostics Github</a>, now <a href="https://pypi.org/project/DeepDiagnostics/">pip installable!</a>
  <p>See also, the SBI tutorial notebooks with accompanying diagnostics created by myself and Jason Poh: <a href="https://colab.research.google.com/drive/1NpwdTy98Ifo-vPuI5Rt-HTFTalgP3cZH?usp=sharing">colab notebook for strong lens SBI</a> and <a href="https://colab.research.google.com/drive/1CRBQqSim3KZV6s5hcwz-zMVmbJzTeaWz?usp=sharing">colab notebook for linear regression</a> as well as the <a href="files/SBI_talk_COFI.pdf">slides from an SBI tutorial talk</a> I gave at a physics winter school.</p>

<hr>

<p><b>DeepBench: Flexible benchmark datasets for machine learning research</b> 
<p>In order to evaluate the performance of ML tools for inference, I rely upon benchmark datasets.
In Deepskies lab, we have developed our own benchmark utilities (DeepBench), 
which produces simple astrophysical objects in a flexible framework that allows the user to control the noise properties.
Using simple tools like a pendulum with stochastic noise,
I am creating a methodology for comparing the bias and uncertainty in parameter estimation from various inference techniques. This type 
of simple dataset also allows me to experiment with comparing hierarchical to non-hierarchical inference paradigms (below).
<img src="files/one_pendulum.png" width=500 class="center">
<p><a href="https://github.com/deepskies/DeepBench">DeepBench Github</a>

<hr>

<p><b>Hierarchical Bayesian inference and simulation based inference (SBI)</b>
  <p>Hierarchical inference is an important tool for estimating parameters of individual data units as well as global parameters.
  This is especially relevant to estimating cosmological parameters, for instance in the case of multiple strong lenses in an all sky imaging survey; the goal is to estimate the individual lens parameters as well as global cosmological parameters.
  <p>Using the above uncertainty framework and DeepBench tools coupled with numpyro, I compare various state-of-the-art inference techniques such as <b>hierarchical inference (sampling)</b> and <b>simulation based inference</b>.
  <img src="files/hierarchical_pendulum.png" width=500 class="center">
  <p>^Using the benchmark pendulum dataset to compare the performance of hierarchical sampling and SBI.</p>
  <p>Here's a link to a <a href="https://colab.research.google.com/drive/1Be1QMo4mTM-Hq89yRvmi0AHHO4vgH5x3?usp=sharing">Google colab</a> notebook example of running Bayesian inference on a simple linear fit using the Hamiltonian Monte Carlo capabilities of numpyro. </p>

  <hr>

<p><b>Harnessing machine learning to improve Chandra HRC</b>                                                                                                                                           
<p>I worked closely with Chandra leadership to develop a machine learning framework, <a href="https://github.com/beckynevin/EVE">EVE</a>, to distinguish between real X-ray events and background events (such as cosmic rays). An improved background identification technique is highly valued by the X-ray community, as it would greatly improve the S/N of future and archival Chandra observations. However, this is a huge challenge, as X-ray events are highly multi-dimensional (i.e., each event has information about the time of arrival, position, and energy) and we lack labelled synthetic data.</p>
<img src="files/Chandra_ML.png" width=500 class="center">
<p> With EVE, I pursued multi-pronged approach for background classification, using multiple options for machine learning algorithms such as a supervised random forest and a semi-supervised bagging classifier that learns from X-ray observations on the fly. The figure above shows an example of the semi-supervised technique assigning probability values to events.


    <hr>

      
      </div>




        </script>

      </div>



    </div>
  </body>
</html>
